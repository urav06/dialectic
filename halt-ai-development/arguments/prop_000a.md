{
  "id": "prop_000a",
  "side": "prop",
  "exchange": 0,
  "title": "Irreversibility Demands Precaution Before Deployment",
  "claim": "The potentially irreversible and catastrophic nature of AGI risks justifies pausing development until safety frameworks can address extinction-level scenarios.",
  "attacks": [],
  "defends": []
}

## Claim

The potentially irreversible and catastrophic nature of AGI risks justifies pausing development until safety frameworks can address extinction-level scenarios.

## Grounds

### 1. 2023 Statement signed by AI researchers and industry leaders including Hinton, Bengio, Altman, and Hassabis (https://www.safe.ai/work/statement-on-ai-risk)

> Hundreds of AI experts and notable figures signed a statement declaring that 'mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.' A 2022 survey of AI researchers found that the majority believed there is a 10 percent or greater chance that human inability to control AI will cause an existential catastrophe.

**Relevance:** The very researchers building AGI acknowledge it poses extinction-level risks comparable to nuclear war, establishing the catastrophic stakes.

### 2. Logical principle of irreversibility in risk management

> Extinction and civilizational collapse are uniquely irreversible outcomes. Unlike recoverable harms where we can learn from mistakes and adjust, an existential catastrophe permits no iteration, no correction, no second attempt. The asymmetry between reversible and irreversible harms demands fundamentally different risk calculi.

**Relevance:** Standard risk-benefit frameworks fail when outcomes include permanent elimination of all future value; precaution becomes rational necessity.

### 3. Stanford economic analysis on AI existential risk (https://web.stanford.edu/~chadj/existentialrisk.pdf)

> Economic analysis demonstrates that even low-probability catastrophic outcomes justify substantial investments in safety research. The study concludes that current global efforts in AI safety and alignment research are vastly insufficient relative to the scale and urgency of existential risks posed by transformative AI.

**Relevance:** Rigorous economic modeling confirms that rational actors should prioritize safety given the magnitude of potential loss.

## Warrant

When facing potentially irreversible catastrophic outcomes acknowledged by domain experts, the burden of proof shifts to demonstrating safety before proceeding, not demonstrating harm after the fact.

## Backing

This precautionary logic underlies established regulatory frameworks for nuclear technology, where deployment required prior safety demonstration rather than post-hoc harm documentation.

## Qualifier

For systems approaching human-level general intelligence