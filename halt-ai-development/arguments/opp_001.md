{
  "id": "opp_001",
  "side": "opp",
  "exchange": 1,
  "title": "Safety Research Requires Continued Capability Development",
  "claim": "Pausing AGI development would halt the empirical safety research that produces the very frameworks the motion demands, creating a paradox that prevents its own resolution.",
  "attacks": [
    {
      "target_id": "prop_000c",
      "type": "warrant_attack"
    },
    {
      "target_id": "prop_000b",
      "type": "grounds_attack"
    },
    {
      "target_id": "prop_000a",
      "type": "warrant_attack"
    }
  ],
  "defends": [
    {
      "target_id": "opp_000a",
      "type": "reinforce"
    }
  ]
}

## Claim

Pausing AGI development would halt the empirical safety research that produces the very frameworks the motion demands, creating a paradox that prevents its own resolution.

## Grounds

### 1. Anthropic, 'Core Views on AI Safety' (https://www.anthropic.com/news/core-views-on-ai-safety)

> Anthropic argues that 'large models are qualitatively different from smaller models' and that safety concerns 'might only arise with near-human-level systems.' Certain safety approaches like Constitutional AI 'can only work on large models.' They explicitly warn against letting 'excessive caution make it so that the most safety-conscious research efforts only ever engage with systems that are far behind the frontier, thereby dramatically slowing down what we see as vital research.'

**Relevance:** The leading safety-focused AI lab explicitly argues that frontier development is prerequisite to developing the safety frameworks the motion demands.

### 2. Iterative alignment research paradigm, 2024-2025 empirical developments

> Modern alignment techniques including RLHF, Constitutional AI, scalable oversight, and automated alignment research are empirical methods requiring iterative testing on capable systems. Safety research has shifted from theoretical speculation to empirical science, with approaches like debate, recursive reward modeling, and interpretability research requiring frontier models to validate effectiveness.

**Relevance:** Safety frameworks emerge from empirical research on capable systems, not from theoretical work conducted during capability freezes.

### 3. OpenAI, 'How we think about safety and alignment' (2025)

> OpenAI's approach treats safety as empirical science requiring iterative deployment: 'Achieving safe AGI demands engaging with reality, testing systems in the real world. Instead of working backward from hypothetical end states, they move forward taking incremental steps, while measuring and testing continuously.'

**Relevance:** Major developers converge on the view that safety emerges through iterative real-world testing, not pre-deployment theorizing.

## Warrant

The motion presupposes that safety frameworks can be developed independently of capability advancement. But empirical safety research is parasitic on capability research: you cannot learn to align systems that do not yet exist. A pause freezes safety research alongside capability research, perpetuating the gap the motion claims to address.

## Backing

This mirrors the relationship between aircraft development and aviation safety: crash testing, structural analysis, and safety protocols evolved through building and testing planes, not through theoretical work conducted before first flight.

## Qualifier

For empirically-grounded safety approaches

## Attacks

### Attacking prop_000c

**Type:** warrant_attack

The Asilomar analogy fails on structural grounds. Biotechnology in 1975 was dominated by academic researchers with shared epistemic norms and minimal commercial stakes. AI development is dominated by corporations with massive profit incentives, global competitive pressure, and no mechanism for universal compliance. Harvard International Review (2024) notes that 'the majority of AI developers are contracted to private companies, blurring public responsibility and private sector work.' Asilomar's voluntary self-restraint succeeded because participants had aligned interests; AI developers fundamentally do not.

### Attacking prop_000b

**Type:** grounds_attack

The proposition cites alignment difficulties as grounds for pause, but this inverts the causal logic. Anthropic's alignment faking research, Leike's resignation, and the FLI Safety Index are products of ongoing capability development. Each safety failure identified emerges from deploying and testing systems. Pausing would not accelerate solutions to these problems; it would freeze our understanding at current inadequate levels while preventing the empirical work needed to improve.

### Attacking prop_000a

**Type:** warrant_attack

The proposition's precautionary principle assumes we can develop safety frameworks during a pause. But the irreversibility argument cuts both ways: if AGI risks are unpredictable before deployment, so are AGI safety properties. We cannot 'demonstrate safety before proceeding' for systems whose behavior cannot be characterized until they exist. The only path to safety knowledge runs through careful, staged development with empirical testing at each phase.

## Defends

### Defending opp_000a

**Type:** reinforce

The proposition has not answered how 'robust safety frameworks are established' would be determined. Our argument that the pause condition is unfalsifiable is strengthened by the empirical safety research paradigm: if safety frameworks require frontier models to develop and validate, demanding frameworks before development creates an unresolvable circularity. The condition cannot be satisfied because satisfaction requires the very activity the condition prohibits.