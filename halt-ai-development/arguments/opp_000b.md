{
  "id": "opp_000b",
  "side": "opp",
  "exchange": 0,
  "title": "Unilateral Pause Concentrates Power Adversarially",
  "claim": "A pause by safety-conscious actors transfers AGI development leadership to actors with fewer safety commitments, increasing rather than decreasing catastrophic risk.",
  "attacks": [],
  "defends": []
}

## Claim

A pause by safety-conscious actors transfers AGI development leadership to actors with fewer safety commitments, increasing rather than decreasing catastrophic risk.

## Grounds

### 1. RAND Corporation (2025), 'China's AI Models Are Closing the Gap'

> The performance gap between the best Chinese and U.S. AI models shrank from 9.3% in 2024 to 1.7% in February 2025. DeepSeek developed its first model under U.S. technology restrictions, serving as 'a canary in the coal mine, warning that there is a limit to what trying to stymie China's high-technology sector can achieve.' U.S. export controls contained specification errors that allowed workarounds enabling DeepSeek's rise.

**Relevance:** Demonstrates that restrictions on leading developers do not halt global progress but shift development to less constrained actors.

### 2. Brookings Institution (2024), 'How Will AI Influence US-China Relations'

> If China or the United States were to more effectively diffuse AI in its economy and gain even a modest improvement in gross domestic product, this could be a decisive advantage for that nation. The national stakes for such an advantage are so high as to make competition likely rather than cooperation on development restrictions.

**Relevance:** Explains why unilateral pause is strategically untenable: the competitive stakes ensure non-compliant actors will continue development.

### 3. Cairo Review of Global Affairs (2025), analysis of US-China AI dynamics

> U.S. policy on AI and China has blocked progress on erecting an international regime of AI safety and security around advanced models. Without Chinese government and company involvement in this process, no globally acceptable guardrails can be established. This could have grave implications for AI safety, especially regarding efforts to mitigate potential existential risks posed by AI.

**Relevance:** Shows that unilateral restriction undermines rather than advances the global safety cooperation the motion ostensibly seeks.

## Warrant

Safety is not achieved by who develops AGI first, but by the values and practices of developers. Pausing actors with strong safety cultures while competitors with weaker safety commitments continue development increases the probability that transformative AI emerges from contexts with fewer safeguards.

## Backing

Game-theoretic analysis of arms races consistently shows that unilateral restraint by one party accelerates the other's advantage without reducing aggregate risk.