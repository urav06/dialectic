{
  "id": "opp_003",
  "side": "opp",
  "exchange": 3,
  "title": "Safety Research Fails Without Scale-Matched Systems",
  "claim": "The proposition's capability-independent safety research paradigm is empirically failing, as leading interpretability techniques cannot scale to frontier models and proposed alternatives remain years from validation.",
  "attacks": [
    {
      "target_id": "prop_002",
      "type": "grounds_attack"
    },
    {
      "target_id": "prop_002",
      "type": "warrant_attack"
    },
    {
      "target_id": "prop_000b",
      "type": "claim_attack"
    }
  ],
  "defends": [
    {
      "target_id": "opp_001",
      "type": "reinforce"
    },
    {
      "target_id": "opp_000c",
      "type": "reinforce"
    }
  ]
}

## Claim

The proposition's capability-independent safety research paradigm is empirically failing, as leading interpretability techniques cannot scale to frontier models and proposed alternatives remain years from validation.

## Grounds

### 1. DeepMind Safety Research (March 2025), 'Negative Results for Sparse Autoencoders on Downstream Tasks and Deprioritising SAE Research'

> Google DeepMind announced in March 2025 they are deprioritizing sparse autoencoder (SAE) research, their leading mechanistic interpretability approach. Testing SAEs on the downstream task of detecting harmful intent in user prompts, they found SAEs underperformed a simple linear probe baseline. The team concluded that while 'interpreting the concepts in LLM activations is a crucial problem,' SAEs 'clearly have issues' and they will 'explore other research directions.'

**Relevance:** The proposition's claim that interpretability research on existing models generates transferable safety knowledge is undermined by the leading safety lab abandoning their primary approach due to empirical failure.

### 2. AI Frontiers (2025), 'The Misguided Quest for Mechanistic AI Interpretability'; Bereska et al. survey of mechanistic interpretability challenges

> A comprehensive review documents that 'today's frontier models (tens or hundreds of billions of parameters) are extraordinarily hard to interpret fully.' As models grow, interpretability methods including activation patching, ablations, and probing 'become computationally expensive and less effective.' DeepMind's 2023 attempt to apply interpretability methods to the 70-billion-parameter Chinchilla model required 'an intensive, months-long effort' yet yielded 'mixed results,' raising 'questions about the practicality of understanding large models in this way.'

**Relevance:** Demonstrates that interpretability insights from smaller models do not transfer reliably to frontier systems, falsifying the proposition's claim that safety research proceeds independently of scale.

### 3. Bengio (June 2025), LawZero announcement; Axios, TechCrunch reporting

> Bengio's LawZero nonprofit, cited by the proposition as proof that capability-independent safety research is viable, has $30 million in funding for an 18-month 'foundational research period' (2025-2026) to 'prove non-agentic AI viability at small scale.' The scaling phase requiring 'frontier-level compute resources' is projected for 2027 and beyond. Bengio acknowledged to Axios he 'needs 10 times' his current funding just for computing resources.

**Relevance:** The proposition's key example of capability-independent safety research is years from producing usable frameworks, far too late to justify a pause today.

## Warrant

The proposition claims a pause enables safety research because substantial work proceeds independently of building more capable systems. But the empirical record shows the opposite: interpretability techniques that worked on small models fail at scale, the leading safety lab has abandoned its primary approach, and proposed alternatives like Scientist AI are years from validation. A pause would freeze safety research at precisely the point where it requires frontier systems to progress.

## Backing

This reflects a general pattern in empirical science: techniques validated at one scale frequently fail when applied to systems of greater complexity. Aviation safety engineering that worked for propeller aircraft required fundamental reconceptualization for jet engines; nuclear reactor safety principles developed for research reactors needed extensive revision for commercial-scale operations.

## Qualifier

For safety approaches targeting frontier-scale risks

## Attacks

### Attacking prop_002

**Type:** grounds_attack

The proposition cites Bengio's LawZero as evidence that serious safety work proceeds without frontier development. But LawZero launched in June 2025 with 18 months of foundational research before any scaling attempt, needs 10x its current funding, and explicitly acknowledges it cannot yet validate its approach. Citing an underfunded startup's aspirations as proof that capability-independent safety research works is like citing a grant proposal as proof of experimental results.

### Attacking prop_002

**Type:** warrant_attack

The proposition's warrant conflates research categories. The motion calls for 'robust safety frameworks' that would govern AGI deployment. Formal verification and interpretability research on small models cannot validate frameworks for systems exhibiting emergent capabilities that, by definition, 'cannot be predicted' from smaller models. The proposition assumes safety properties transfer across scale, but DeepMind's SAE failure demonstrates they often do not.

### Attacking prop_000b

**Type:** claim_attack

The proposition argues that unsolved alignment problems justify a pause. But their own exchange reveals the paradox: they cite alignment faking as a danger discovered through frontier model research, then argue safety research can proceed without frontier models. If alignment faking emerges only at scale, as the Anthropic research showed, how would paused researchers discover analogous problems in future systems? The proposition cannot have it both ways.

## Defends

### Defending opp_001

**Type:** reinforce

The proposition claimed existing frontier models suffice for safety research during a pause. But emergent capabilities are defined as properties that 'cannot be predicted' from smaller systems. The International AI Safety Report warns that 'future improvements in AI capabilities may be discontinuous, meaning that risks could emerge with little warning.' Safety frameworks for AGI require understanding capabilities that by definition only emerge in more capable systems than currently exist.

### Defending opp_000c

**Type:** reinforce

The proposition attacked our opportunity cost argument by claiming unsafe AGI delivers no benefits. This assumes safety and capability are binary states. In reality, AI systems deliver incremental benefits at every capability level while risks scale with deployment scope. A pause sacrifices certain incremental benefits today against speculative catastrophic risks from hypothetical future systems, while the proposition's own evidence shows their safety research alternatives remain unvalidated.