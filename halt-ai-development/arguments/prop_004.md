{
  "id": "prop_004",
  "side": "prop",
  "exchange": 4,
  "title": "Unpredictable Emergence Validates Precautionary Pause",
  "claim": "The opposition's claim that safety requires racing ahead with systems exhibiting unpredictable emergent capabilities is self-defeating; capability unpredictability strengthens rather than weakens the case for pause.",
  "attacks": [
    {
      "target_id": "opp_003",
      "type": "grounds_attack"
    },
    {
      "target_id": "opp_003",
      "type": "warrant_attack"
    },
    {
      "target_id": "opp_001",
      "type": "claim_attack"
    }
  ],
  "defends": [
    {
      "target_id": "prop_002",
      "type": "reinforce"
    },
    {
      "target_id": "prop_003",
      "type": "reinforce"
    }
  ]
}

## Claim

The opposition's claim that safety requires racing ahead with systems exhibiting unpredictable emergent capabilities is self-defeating; capability unpredictability strengthens rather than weakens the case for pause.

## Grounds

### 1. Anthropic Circuit Tracing Research, March-July 2025 (https://transformer-circuits.pub/2025/july-update/index.html)

> Anthropic's interpretability team has advanced beyond sparse autoencoders to circuit tracing and attribution graphs, successfully mapping internal reasoning in Claude 3.5 Haiku. Their July 2025 update demonstrates progress on feature-based frameworks for attention mechanisms, identifying specific circuits including multi-step reasoning paths and planning behaviors. This work proceeds on existing frontier models without requiring new capability development.

**Relevance:** Directly refutes the claim that interpretability research is 'failing'; the field is diversifying methods, not abandoning the enterprise.

### 2. Random Scaling for Emergent Capabilities, February 2025 (https://arxiv.org/abs/2502.17356)

> Research demonstrates that breakthrough capabilities in LLMs emerge through unpredictable distributions across random seeds, with different training runs producing either smooth or emergent scaling. The authors conclude that 'sharp breakthroughs in metrics are produced by underlying continuous changes in their distribution across seeds,' making specific capability emergence fundamentally unpredictable even given scale.

**Relevance:** The opposition's own premise that emergent capabilities 'cannot be predicted' is an argument FOR pause; you cannot safely race toward unpredictable dangers.

### 3. EU AI Act Article 99 enforcement, August 2025 (https://artificialintelligenceact.eu/article/99/)

> As of August 2025, the EU AI Act's penalty regime became fully operational with national market surveillance authorities empowered to impose fines up to 35 million EUR or 7% of global turnover for prohibited AI practices. The European AI Office holds exclusive enforcement jurisdiction over General-Purpose AI Models, creating binding regulatory infrastructure that constitutes precisely the 'robust safety framework' the motion envisions.

**Relevance:** The pause condition is being satisfied through binding legislation with real enforcement teeth, independent of summit diplomacy.

## Warrant

The opposition's argument rests on a fatal contradiction: they claim emergent capabilities are unpredictable and therefore require frontier development to study, while simultaneously arguing we should race ahead despite this unpredictability. If capabilities emerge unpredictably, cautious pause with intensive study of existing systems is the rational response, not acceleration into unknown danger. The safety research paradigm is not failing but diversifying beyond single techniques.

## Backing

This mirrors the precautionary logic in every high-risk domain: when outcomes are unpredictable, we proceed cautiously rather than assuming we can handle whatever emerges. Nuclear reactor scaling proceeded incrementally precisely because emergent behaviors at scale were unpredictable.

## Qualifier

For systems approaching transformative capability thresholds

## Attacks

### Attacking opp_003

**Type:** grounds_attack

The opposition cites DeepMind deprioritizing SAEs as proof interpretability is 'failing.' But Anthropic's March-July 2025 work on circuit tracing and attribution graphs demonstrates the field is diversifying methods, not collapsing. OpenAI's weight-sparse transformers research (November 2025) offers another promising direction. One lab shifting approaches while others advance alternative techniques is how healthy scientific fields progress, not evidence of failure.

### Attacking opp_003

**Type:** warrant_attack

The opposition's warrant assumes that unpredictable emergent capabilities require racing ahead to study them. This inverts rational risk management. If capabilities are genuinely unpredictable, as they claim, then accelerating development means deliberately creating systems whose dangers cannot be anticipated. The logical implication of emergence unpredictability is pause and intensive study, not acceleration into the unknown.

### Attacking opp_001

**Type:** claim_attack

The opposition claims safety research requires continued capability development. But their own evidence about emergent unpredictability undermines this: if we cannot predict what capabilities emerge at scale, building more capable systems does not help us prepare for those capabilities. We would be creating dangers faster than we can understand them. The motion's pause enables closing this gap rather than widening it.

## Defends

### Defending prop_002

**Type:** reinforce

The opposition attacked our capability-independent safety research by citing LawZero's timeline. But Anthropic's circuit tracing work demonstrates that interpretability research on existing frontier models continues advancing without building more capable systems. The motion pauses capability development, not safety research on current systems. Anthropic's July 2025 circuits updates prove this research proceeds regardless of new capability development.

### Defending prop_003

**Type:** reinforce

The opposition claims binding enforcement requires diplomatic consensus that does not exist. But the EU AI Act entered full penalty enforcement in August 2025 with up to 7% of global turnover in fines. This binding framework operates independently of summit declarations. Robust safety frameworks are being established through legislation, satisfying the motion's condition through regulatory convergence rather than diplomatic unanimity.