{
  "id": "opp_002",
  "side": "opp",
  "exchange": 2,
  "title": "International Safety Coordination Is Collapsing, Not Advancing",
  "claim": "The proposition's evidence of emerging international safety frameworks actually demonstrates their breakdown, as the 2025 Paris Summit revealed fundamental geopolitical divergence preventing the coordinated pause the motion requires.",
  "attacks": [
    {
      "target_id": "prop_001",
      "type": "grounds_attack"
    },
    {
      "target_id": "prop_001",
      "type": "warrant_attack"
    },
    {
      "target_id": "prop_000c",
      "type": "grounds_attack"
    }
  ],
  "defends": [
    {
      "target_id": "opp_000b",
      "type": "reinforce"
    },
    {
      "target_id": "opp_000a",
      "type": "reinforce"
    }
  ]
}

## Claim

The proposition's evidence of emerging international safety frameworks actually demonstrates their breakdown, as the 2025 Paris Summit revealed fundamental geopolitical divergence preventing the coordinated pause the motion requires.

## Grounds

### 1. Paris AI Action Summit outcomes, February 2025 (TechCrunch, Semafor)

> The United States and United Kingdom refused to sign the Paris AI Summit declaration calling for 'open, inclusive, transparent, ethical, safe, secure and trustworthy' AI development. Vice President JD Vance explicitly stated: 'I'm not here this morning to talk about AI safety. I'm here to talk about AI opportunity.' The UK declined citing lack of 'practical clarity on global governance' and insufficient attention to 'national security challenges.' Only 61 nations signed the non-binding declaration, with the two leading Western AI powers absent.

**Relevance:** Demonstrates that international AI safety coordination is fragmenting, not converging, making the coordinated pause the motion requires unachievable.

### 2. Dario Amodei, Anthropic CEO statement on Paris Summit (https://www.anthropic.com/news/paris-ai-summit)

> The CEO of the leading safety-focused AI company called the Paris Summit a 'missed opportunity,' warning that 'greater focus and urgency is needed on several topics given the pace at which the technology is progressing.' He criticized the summit for failing to address democratic leadership in AI, security risks, and economic disruption, concluding that international conversations have fallen fundamentally short.

**Relevance:** When the CEO of Anthropic—the most safety-conscious major AI developer—declares international coordination a failure, it undermines proposition claims that frameworks are emerging.

### 3. Seoul Summit Frontier AI Safety Commitments analysis (Montreal AI Ethics Institute, GOV.UK)

> The Seoul Summit commitments cited by the proposition are entirely voluntary, with enforcement relying solely on self-reported transparency and information sharing. No penalties exist for non-compliance. Companies commit only to 'develop' internal accountability frameworks and 'provide public transparency' at their discretion, with explicit carve-outs for 'sensitive commercial information.'

**Relevance:** The 'operationalized' frameworks the proposition cites lack any enforcement mechanism, making them aspirational statements rather than robust safety constraints.

## Warrant

A pause requires coordinated international action to prevent safety-conscious actors from merely ceding leadership to less constrained competitors. The proposition's own evidence—international summits—shows these mechanisms failing, not succeeding. When leading AI powers refuse to sign safety declarations and safety-focused companies call international coordination a 'missed opportunity,' the preconditions for an effective pause do not exist.

## Backing

The trajectory from Bletchley (2023, safety-focused) to Seoul (2024, voluntary commitments) to Paris (2025, US/UK refusal, 'action' over safety) demonstrates deteriorating rather than improving international coordination on AI safety.

## Qualifier

In the current geopolitical environment

## Attacks

### Attacking prop_001

**Type:** grounds_attack

The proposition claims Seoul and Paris summits demonstrate 'measurable safety criteria being operationalized.' The actual Paris Summit outcome was US and UK refusal to sign any declaration, a shift from 'AI Safety' to 'AI Action' framing, and Anthropic's CEO calling it a 'missed opportunity.' The trajectory shows international safety coordination collapsing, not crystallizing. Citing this process as evidence of emerging frameworks is like citing a crumbling alliance as proof of military strength.

### Attacking prop_001

**Type:** warrant_attack

The proposition's warrant that 'substantial international progress' undermines our definitional argument fails because progress toward non-binding, voluntary commitments without enforcement mechanisms does not constitute 'robust safety frameworks.' The CTBT analogy fails precisely because CTBT has mandatory verification with 300+ monitoring stations, while AI commitments have voluntary self-reporting with commercial confidentiality exemptions. These are categorically different governance regimes.

### Attacking prop_000c

**Type:** grounds_attack

The Asilomar precedent requires a cohesive scientific community with aligned interests. The Paris Summit revealed the opposite: the US emphasized 'opportunity' over safety, the UK cited insufficient national security attention, China signed a non-binding declaration, and Europe pursued a third path. There is no unified community capable of voluntary self-restraint. The 2025 outcome falsifies the proposition's assumption that Asilomar-style coordination can emerge.

## Defends

### Defending opp_000b

**Type:** reinforce

The proposition claimed our unilateral pause argument was undermined by Chinese participation at Seoul and Paris. But China signed only non-binding declarations while the US and UK—the leading Western AI powers—refused even these minimal commitments. The result is precisely what we predicted: fragmentation that advantages actors unconstrained by safety norms, not coordinated restraint.

### Defending opp_000a

**Type:** reinforce

The proposition's attack on our definitional argument cited compute thresholds and the Seoul process. But voluntary commitments with self-defined compliance and commercial confidentiality exemptions cannot constitute 'robust frameworks.' When the pause condition is 'when we collectively agree frameworks are robust,' and leading nations cannot even agree on a non-binding declaration, the condition remains indefinitely unsatisfied.