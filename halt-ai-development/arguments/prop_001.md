{
  "id": "prop_001",
  "side": "prop",
  "exchange": 1,
  "title": "Measurable Safety Criteria Already Exist and Proliferate",
  "claim": "Contrary to opposition claims, objective and measurable criteria for AI safety frameworks already exist and are being operationalized by governments and international bodies.",
  "attacks": [
    {
      "target_id": "opp_000a",
      "type": "grounds_attack"
    },
    {
      "target_id": "opp_000a",
      "type": "warrant_attack"
    },
    {
      "target_id": "opp_000b",
      "type": "grounds_attack"
    }
  ],
  "defends": [
    {
      "target_id": "prop_000c",
      "type": "reinforce"
    }
  ]
}

## Claim

Contrary to opposition claims, objective and measurable criteria for AI safety frameworks already exist and are being operationalized by governments and international bodies.

## Grounds

### 1. Seoul Ministerial Statement, AI Seoul Summit, May 2024 (https://www.gov.uk/government/news/new-commitmentto-deepen-work-on-severe-ai-risks-concludes-ai-seoul-summit)

> At the AI Seoul Summit, governments agreed for the first time to develop shared risk thresholds for frontier AI development and deployment, including specific criteria for when model capabilities pose 'severe risks' requiring mitigation. Sixteen AI companies signed the Frontier AI Safety Commitments, establishing concrete safety obligations. Specific thresholds were set to be established before the February 2025 Paris summit.

**Relevance:** Demonstrates that international consensus on measurable risk thresholds is actively being achieved, refuting the claim that such criteria are impossible.

### 2. Conditional AI Safety Treaty proposal, March 2025 (https://arxiv.org/html/2503.18956v1)

> Academic and policy researchers have proposed compute thresholds measured in floating-point operations (10^21 to 10^25 FLOP) as regulatory triggers. Verification mechanisms include hardware-based monitoring with embedded chip identifiers, routine and challenge inspections of compute facilities, remote sensing via satellite imagery and energy consumption monitoring, and financial intelligence tracking hardware movement through customs data.

**Relevance:** Provides concrete, quantifiable metrics and verification mechanisms directly analogous to nuclear treaty verification.

### 3. Comprehensive Nuclear-Test-Ban Treaty verification system (https://www.un.org/en/un-chronicle/ending-nuclear-testing-advance-global-peace-and-security)

> The CTBT established an International Monitoring System with over 300 facilities worldwide using seismic, hydroacoustic, infrasound, and radionuclide monitoring. This system has successfully detected all six nuclear tests conducted this century, demonstrating that even for technologies where 'robust safety' seems unmeasurable, practical verification regimes can be constructed and operationalized.

**Relevance:** Provides precedent that verification systems for existentially dangerous technologies are achievable despite initial claims of impossibility.

## Warrant

The opposition's claim that 'robust safety frameworks' lacks measurable criteria ignores the substantial international progress already made in defining quantifiable thresholds, verification mechanisms, and compliance standards for AI safety governance.

## Backing

Every major international safety regime—nuclear, chemical, biological—faced initial objections that verification was impossible, yet practical systems were developed through iterative refinement and international cooperation.

## Qualifier

For frontier AI systems above defined compute thresholds

## Attacks

### Attacking opp_000a

**Type:** grounds_attack

The opposition cites the difficulty of measuring 'robustness' abstractly, but ignores that practical AI safety governance uses concrete proxies: compute thresholds in FLOP, capability benchmarks, red-team evaluation scores, and safety incident rates. The GovAI paper they cite acknowledges these challenges exist but does not conclude they are insurmountable—it calls for continued development, not abandonment of the enterprise.

### Attacking opp_000a

**Type:** warrant_attack

The leap from 'measurement is difficult' to 'pause becomes permanent prohibition' is a non sequitur. The CTBT took decades to develop verification systems but this did not make the goal of ending nuclear testing illegitimate. Iterative improvement of safety criteria during a pause is precisely what the motion envisions—the pause creates conditions for developing the very standards the opposition claims do not exist.

### Attacking opp_000b

**Type:** grounds_attack

The opposition assumes a pause must be unilateral, but the Bletchley and Seoul summits included China as a signatory. The February 2025 Paris AI Action Summit continued this multilateral engagement. The motion calls for establishing frameworks—a process that inherently involves international coordination, not unilateral Western restraint while competitors proceed unchecked.

## Defends

### Defending prop_000c

**Type:** reinforce

Our Asilomar precedent is strengthened by the ongoing Seoul-Paris AI safety process, which demonstrates that international coordination on safety frameworks for transformative technologies is not merely historical possibility but active present reality. Just as Asilomar created NIH Guidelines, the current process is creating measurable thresholds and verification mechanisms.